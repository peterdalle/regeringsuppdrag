{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ladda ned PDF regeringsuppdrag\n",
    "\n",
    "Sidan https://www.regeringen.se/regeringsuppdrag/ anropar en [JSON-fil](https://www.regeringen.se/Filter/GetFilteredItems?filterType=Taxonomy&filterByType=FilterablePageBase&preFilteredCategories=1342&rootPageReference=0&page=1&pageSize=500&displayLimited=false&sortAlphabetically=False) dynamiskt vid vaje anrop. Genom att manipulera `pageSize` i URL:en kan man plocka ut 500 länkar åt gången och sedan skrapa länkens HTML (JSON-filen innehåller HTML som värde) och därifrån hämta länken till PDF-filen som man sedan laddar ned.\n",
    "\n",
    "Not: Med tanke på att JSON-filen verkar vara generell för allt möjligt innehåll kan man nog ladda ned fler PDF-filer än bara regeringsuppdrag (se exempelvis `preFilteredCategories=1342` i URL:en). Har dock inte testat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "from lxml import html\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# skrapar html\n",
    "def scrapewebpage(url):\n",
    "    UseSSL = True  # Om du får SSLError, ändra detta till False.\n",
    "    if UseSSL:\n",
    "        web = request.urlopen(url)\n",
    "    else:\n",
    "        web = request.urlopen(url, context=ssl._create_unverified_context())\n",
    "    if web.getcode() == 200:\n",
    "        return(web.read())\n",
    "    else:\n",
    "        print(\"Error %s reading %s\" % str(web.getcode()), url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utforska\n",
    "\n",
    "Se vad JSON-filen innehåller för något smått och gott."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skrapa json-fil\n",
    "data = scrapewebpage(\"https://www.regeringen.se/Filter/GetFilteredItems?filterType=Taxonomy&filterByType=FilterablePageBase&preFilteredCategories=1342&rootPageReference=0&page=1&pageSize=500&displayLimited=false&sortAlphabetically=False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message\n",
      "TotalCount\n"
     ]
    }
   ],
   "source": [
    "# läs in json\n",
    "import json\n",
    "j = json.loads(data)\n",
    "\n",
    "# nycklar i json-datan\n",
    "for key in j.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2190"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# regeringsuppdrag totalt\n",
    "j[\"TotalCount\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plocka ut alla länkar på sidan från j[\"Message\"] som innehåller html\n",
    "soup = BeautifulSoup(j[\"Message\"], \"lxml-xml\")\n",
    "links = soup.find_all(\"a\", \"readmore\")\n",
    "len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/regeringsuppdrag/2018/06/uppdrag-till-polismyndigheten-och-aklagarmyndigheten-om-forbattrad-hantering-av-bidragsbrott/'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# kolla hur url:erna till regeringsuppdragen ser ut\n",
    "url = links[0].get(\"href\")\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hämta egeringsuppdragssidornas html\n",
    "html = scrapewebpage(\"https://regeringen.se\" + url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'  <ul class=\"list--Block--icons\" >\\r\\n\\r\\n                                    <li>            <a href=\"/49cf68/contentassets/5deecb383435428b97154777223c0017/uppdrag-till-polismyndigheten-och-aklagarmyndigheten-om-forbattrad-hantering-av-bidragsbrott.pdf\">Uppdrag till Polismyndigheten och &#197;klagarmyndigheten om f&#246;rb&#228;ttrad hantering av bidragsbrott (pdf 86 kB)</a>\\r\\n</li>\\r\\n                    </ul>\\r\\n\\r\\n\\r\\n                \\r\\n\\r\\n\\r\\n                \\r\\n\\r\\n\\r\\n                \\r\\n                \\r\\n                \\r\\n                \\r\\n                \\r\\n                <div class=\"has-wordExplanation\">\\r\\n                    <p>Polismyndigheten och \\xc3\\x85klagarmyndigheten ska samr\\xc3\\xa5da med de akt\\xc3\\xb6rer som \\xc3\\xa4r relevanta f\\xc3\\xb6r att \\xc3\\xa5stadkomma en effektivare och f\\xc3\\xb6rb\\xc3\\xa4ttrad hantering av bidragsbrott, framf\\xc3\\xb6r allt Ekobrottsmyndigheten, F\\xc3\\xb6rs\\xc3\\xa4kringskassan och \\xc3\\xb6vriga utbetalande myndigheter.</p>\\n<p>Uppdraget ska samordnas av Polismyndigheten. En gemensam delredovisning g\\xc3\\xa4llande analysen av hur handl\\xc3\\xa4ggningen kan f\\xc3\\xb6rb\\xc3\\xa4ttras och effektiviseras ska l\\xc3\\xa4mnas till Justitiedepartementet senast den 21 oktober 2019. En gemensam slutredovisning av planerade och vidtagna \\xc3\\xa5tg\\xc3\\xa4rder och dess effekter p\\xc3\\xa5 utredning och uppklaring av brotten ska l\\xc3\\xa4mnas till Justitiedepartementet senast den 19 oktober 2020.</p>\\r\\n                </div>\\r\\n                \\r\\n                \\r\\n                \\r\\n\\r\\n                \\r\\n                \\r\\n\\r\\n\\r\\n            </div>\\r\\n\\r\\n            <div class=\"col-2\">\\r\\n                \\r\\n            </div>\\r\\n            <div class=\"contentfooter\">\\r\\n<div class=\"date-publ-updated\">\\r\\n    <p><span class=\"published\">Publicerad <time datetime=\"11 juni 2018\">11 juni 2018</time></span> </p>\\r\\n</div>\\r\\n                \\r\\n    <div class=\"island island--primary island--socialmedia rs_skip\">\\r\\n        <h2 class=\"h-underlined h-underlined--beta\">Dela</h2>\\r\\n        <ul class=\"list--icons list--iconsInline\">\\r\\n            <li class=\"list--icons__facebook\"><a href=\"https://www.facebook.com/sharer/sharer.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# kolla var \".pdf\" förekommer\n",
    "pos = str(html).lower().find(\".pdf\")\n",
    "html[pos-1000:pos+1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Försök parsa PDF-fil och metadata från HMTL\n",
    "\n",
    "Nu när vi vet ungefär vad JSON-filen innehåller och hur HTML-sidorna är strukturerade, skapa funktioner för att hantera pdf-filernas namn, hämta ut metadata m.m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skapa några helper-funktioner\n",
    "import urllib.request\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    "\n",
    "# ta ut pdf-filnamnet (antar att första träffen på \".pdf\" är rätt)\n",
    "def get_pdf(pdf_soup):\n",
    "    for a in pdf_soup.find_all(\"a\"):\n",
    "        if a != None:\n",
    "            url = a.get(\"href\")\n",
    "            if url.lower().find(\".pdf\") > 0:\n",
    "                return(\"https://regeringen.se\" + url)\n",
    "    # okej, hittade inget .pdf - gör då några desperata gissningar var pdf-filen finns\n",
    "    url = [div.find(\"a\") for div in pdf_soup.findAll(\"\", \"col-1\")]\n",
    "    if url != None:\n",
    "        return(\"https://regeringen.se\" + url[0].get(\"href\"))\n",
    "    # hittade verkligen inget, ge upp och gråt\n",
    "    return(\"\")\n",
    "\n",
    "# ta ut pdf-filnamnet från url\n",
    "def get_filename(url):\n",
    "    if url.find(\"/\"):\n",
    "        return(url.rsplit(\"/\", 1)[1])\n",
    "\n",
    "# ladda ned pdf\n",
    "def download_pdf(url):\n",
    "    urllib.request.urlretrieve(url, get_filename(url))\n",
    "    \n",
    "# ta ut övrig metadata från regeringsuppdragets sida (titel, diarienummer, beskrivning, datum etc)\n",
    "def get_metadata(url, pdf_soup):\n",
    "    title = \"\"\n",
    "    dnr = \"\"\n",
    "    ingress = \"\"\n",
    "    beskrivning = \"\"\n",
    "    pub = \"\"\n",
    "    h1 = pdf_soup.find(\"h1\").get_text().strip()\n",
    "    try:\n",
    "        dnr = h1.split(\"\\n\")[1].strip()\n",
    "        title = h1.split(\"\\n\")[0].strip()\n",
    "    except:\n",
    "        title = h1\n",
    "        dnr = \"\"\n",
    "    try:\n",
    "        ingress = pdf_soup.find(\"\", \"ingress has-wordExplanation\").get_text().strip()\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        explanation = pdf_soup.find(\"\", \"has-wordExplanation\").get_text().strip()\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        pub = pdf_soup.find(\"time\").get(\"datetime\")\n",
    "    except:\n",
    "        pass\n",
    "    return({\"titel\": title, \"dnr\": dnr, \"ingress\": ingress, \"beskrivning\": explanation,  \"pub\": pub, \n",
    "            \"pdf\": get_filename(get_pdf(pdf_soup)), \"url\": url})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'titel': 'Uppdrag om säker och effektiv tillgång till grunddata',\n",
       " 'dnr': '',\n",
       " 'ingress': '',\n",
       " 'beskrivning': 'Regeringen gerBolagsverket, Lantmäteriet och Skatteverket i uppdrag att tillsammans lämna förslag som syftar till att skapa en säker och effektiv till\\xadgång till grunddata, genom att bl.a. tydliggöra ansvaret för och öka standardi\\xadseringen av sådan data.\\nUppdraget ska slutredovisas till regeringen senast den 30 april 2019.',\n",
       " 'pub': '07 juni 2018',\n",
       " 'pdf': 'uppdrag-om-saker-och-effektiv-tillgang-till-grunddata.pdf',\n",
       " 'url': 'https://regeringen.se/regeringsuppdrag/2018/06/uppdrag-om-saker-och-effektiv-tillgang-till-grunddata/'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# kolla att det verkar funka på ett regeringsuppdrag\n",
    "url = \"https://regeringen.se/regeringsuppdrag/2018/06/uppdrag-om-saker-och-effektiv-tillgang-till-grunddata/\"\n",
    "html = scrapewebpage(url)\n",
    "pdf_soup = BeautifulSoup(html, \"lxml-xml\")\n",
    "get_metadata(url, pdf_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'titel': 'Uppdrag till Arbetsförmedlingen om en modell för upphandlad matchning till etableringsjobb',\n",
       " 'dnr': 'Diarienummer: A2018/01212/A',\n",
       " 'ingress': 'Regeringen uppdrar åt Arbetsförmedlingen att analysera förutsättningarna för och utreda den närmare utformningen av en modell för upphandlad matchning till etableringsjobb. Uppdraget ska genomföras i samråd med LO, Unionen och Svenskt Näringsliv.',\n",
       " 'beskrivning': 'Arbetsförmedlingen ska analysera hur tjänsten Stöd och matchning kan utvecklas och hur tjänsten kan användas tillsammans med andra insatser för att skapa en effektiv matchning till etableringsjobb. Arbetsförmedlingen ska utifrån detta perspektiv analysera dimensioneringen av tjänsten.\\nUtifrån analysen ska Arbetsförmedlingen redovisa hur utformningen av en upphandlad modell för matchning till etableringsjobb skulle kunna se ut. Om det bedöms finnas behov av författningsändringar för att kunna utforma en effektiv modell för upphandlad matchning till etableringsjobb ska sådant förslag redovisas.\\nUppdraget ska redovisas till regeringen (Arbetsmarknadsdepartementet) senast den 30 november 2018. Arbetsförmedlingen ska fortlöpande hålla Arbetsmarknadsdepartementet informerat om uppdragets genomförande.',\n",
       " 'pub': '04 juni 2018',\n",
       " 'pdf': 'uppdrag-till-arbetsformedlingen-om-en-modell-for-upphandlad-matchning-till-etableringsjobb',\n",
       " 'url': 'https://regeringen.se/regeringsuppdrag/2018/06/uppdrag-till-arbetsformedlingen-om-en-modell-for-upphandlad-matchning-till-etableringsjobb/'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vissa regeringsuppdrag har inget .pdf i slutet av filnamnet, som den här,\n",
    "# kolla att den kan parsa ut pdf-filen ändå\n",
    "url = \"https://regeringen.se/regeringsuppdrag/2018/06/uppdrag-till-arbetsformedlingen-om-en-modell-for-upphandlad-matchning-till-etableringsjobb/\"\n",
    "html = scrapewebpage(url)\n",
    "pdf_soup = BeautifulSoup(html, \"lxml-xml\")\n",
    "get_metadata(url, pdf_soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skapa sqlite3 databas\n",
    "\n",
    "Någonstans måste vi ju spara allt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 as lite\n",
    "con = lite.connect(\"regeringsuppdrag.db\")\n",
    "cur = con.cursor()    \n",
    "\n",
    "# skapa databas om den inte redan finns\n",
    "cur.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "counter = cur.fetchall()\n",
    "if len(counter) < 2:\n",
    "    cur.execute(\"CREATE TABLE regeringsuppdrag (id INTEGER PRIMARY KEY AUTOINCREMENT, titel TEXT, dnr TEXT, ingress TEXT, beskrivning TEXT, pub TEXT, pdf TEXT, url TEXT)\")\n",
    "    \n",
    "# dumpa json till sqlite\n",
    "def write_metadata(j):\n",
    "    with con:\n",
    "        cur = con.cursor()\n",
    "        cur.execute(\"INSERT INTO regeringsuppdrag (titel, dnr, ingress, beskrivning, pub, pdf, url) VALUES (?, ?, ?, ?, ?, ?, ?)\",\n",
    "                    (j[\"titel\"], j[\"dnr\"], j[\"ingress\"], j[\"beskrivning\"], j[\"pub\"], j[\"pdf\"], j[\"url\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skrapa allt\n",
    "\n",
    "Nu verkar allt fungera att hämta.\n",
    "\n",
    "Detta är huvudfunktion för att skrapa alla pdf:er. Notera att det är 1,5 sekunders fördröjning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sida 1\n",
      "Laddar ned pdf 1 2 3 4 5 6 "
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Peter\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2971: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import time\n",
    "import sys\n",
    "import math\n",
    "i = 0\n",
    "errors = 0\n",
    "        \n",
    "# 2190 regeringsuppdrag / 500 per sida = 5 sidor,\n",
    "# vi behöver med andra ord köra for-loopen 5 gånger\n",
    "for page in range(1, math.ceil(int(j[\"TotalCount\"]) / 500) + 1): \n",
    "    print(\"Sida {0}\".format(page))\n",
    "    # skrapa 500 resultat åt gången och ta ut alla (förhoppningsvis) 500 länkar\n",
    "    data = scrapewebpage(\"https://www.regeringen.se/Filter/GetFilteredItems?filterType=Taxonomy&filterByType=FilterablePageBase&preFilteredCategories=1342&rootPageReference=0&page=\" + str(page) + \"&pageSize=500&displayLimited=false&sortAlphabetically=False\")\n",
    "    j = json.loads(data)    \n",
    "    soup = BeautifulSoup(j[\"Message\"], \"lxml-xml\")\n",
    "    links = soup.find_all(\"a\", \"readmore\")\n",
    "    \n",
    "    # skrapa länk för länk med regeringsuppdrag\n",
    "    print(\"Laddar ned pdf\", end=\" \")\n",
    "    for a in links:\n",
    "        if a != None:\n",
    "            url = a.get(\"href\")\n",
    "            try:\n",
    "                # ta ut varje sida med regeringsuppdrag & ladda ned pdf\n",
    "                i += 1\n",
    "                html = scrapewebpage(\"https://regeringen.se\" + url)\n",
    "                pdf_soup = BeautifulSoup(html, \"lxml-xml\")\n",
    "                pdf_url = get_pdf(pdf_soup)\n",
    "                if pdf_url != \"\":\n",
    "                    #print(\"Laddar ned https://regeringen.se\" + pdf_url)\n",
    "                    print(i, end=\" \")\n",
    "                    write_metadata(get_metadata(url, pdf_soup))\n",
    "                    download_pdf(pdf_url)\n",
    "                    time.sleep(1.5) # Fördröjning för att vara snäll mot regeringen (maskinerna kommer ta över världen ändå en vacker dag)\n",
    "                else:\n",
    "                    print()\n",
    "                    print(\"hittade ingen pdf: https://regeringen.se\" + url)\n",
    "            except KeyboardInterrupt:\n",
    "                con.close()\n",
    "                sys.exit()\n",
    "            except:\n",
    "                errors += 1\n",
    "                print()\n",
    "                print(\"Fel: https://regeringen.se\" + url + \" \" + str(sys.exc_info()[0]))\n",
    "                  \n",
    "con.close()\n",
    "print()\n",
    "print()\n",
    "print(\"Klart, {0} pdf:er nedladdat, {1} fel\".format(i, errors))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
